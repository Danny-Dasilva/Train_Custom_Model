{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EdgeTPU with Keras",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Danny-Dasilva/Train_Custom_Model/blob/master/EdgeTPU_with_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auEGoMDKW8Z7",
        "colab_type": "text"
      },
      "source": [
        "Build a model by using Keras and convert it to the Edge TPU tflite file.\n",
        "\n",
        "### Install EdgeTPU Compiler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLXmBB9bQoT7",
        "colab_type": "code",
        "outputId": "712acf02-69e3-445e-91e7-5b68658172c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "%%bash\n",
        "\n",
        "echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 6A030B21BA07F4FB\n",
        "\n",
        "sudo apt update > /dev/null\n",
        "sudo apt install edgetpu > /dev/null"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\n",
            "Executing: /tmp/apt-key-gpghome.S5fmFJRlUr/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 6A030B21BA07F4FB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Warning: apt-key output should not be parsed (stdout is not a terminal)\n",
            "gpg: key 6A030B21BA07F4FB: public key \"Google Cloud Packages Automatic Signing Key <gc-team@google.com>\" imported\n",
            "gpg: Total number processed: 1\n",
            "gpg:               imported: 1\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "\n",
            "WARNING: apt does not have a stable CLI interface. Use with caution in scripts.\n",
            "\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 5.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDYJKuhfRoyo",
        "colab_type": "text"
      },
      "source": [
        "## Edge TPU with Keras\n",
        "\n",
        "build very simple model in this notebook.\n",
        "\n",
        "- data: Fashion MNISt\n",
        "- input shape: 28 x 28\n",
        "- output shape: 10\n",
        "- hidden layers: only 1 dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aluRnFKRN9y",
        "colab_type": "code",
        "outputId": "bdcb3648-9cc8-4299-d1b9-8daf5dd0188e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import np_utils\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p40JjhzMSU92",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "e4cdbe2d-02e2-4cea-f6b2-53bdb4fb4230"
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n",
        "\n",
        "testLabels = testY\n",
        "trainLabels = trainY\n",
        "\n",
        "trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))\n",
        "testX = testX.reshape((testX.shape[0], 28, 28, 1))\n",
        "#print(trainX[1])\n",
        "\n",
        "trainX = trainX / 255.0\n",
        "testX = testX / 255.0\n",
        "# one-hot encode the training and testing labels\n",
        "#print(trainX[1])\n",
        "\n",
        "trainY = np_utils.to_categorical(trainY, 10)\n",
        "testY = np_utils.to_categorical(testY, 10)\n",
        "\n",
        "# initialize the label names\n",
        "labelNames = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\",\n",
        "\t\"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhC7xCG6S-yD",
        "colab_type": "text"
      },
      "source": [
        "### Build the model\n",
        "\n",
        "- define build_keras_model function since we have to build model 2 times (for train and eval)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNuqMwp5f_GX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Dense\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "\n",
        "imgindex = 11\n",
        "\n",
        "NUM_EPOCHS = 12\n",
        "INIT_LR = 1e-2\n",
        "BS = 256\n",
        "chanDim = -1\n",
        "classes = 10\n",
        "\n",
        "def build_keras_model():\n",
        "  \n",
        "    return keras.Sequential([\n",
        "            keras.layers.Conv2D(32, (3, 3), padding=\"same\", input_shape=(28,28,1)),\n",
        "            keras.layers.Activation(\"relu\"),\n",
        "            keras.layers.BatchNormalization(axis=chanDim, fused=False),\n",
        "            keras.layers.Conv2D(32, (3, 3), padding=\"same\"),\n",
        "            keras.layers.Activation(\"relu\"),\n",
        "            keras.layers.BatchNormalization(axis=chanDim, fused=False),\n",
        "            keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "            keras.layers.Dropout(0.25),\n",
        "            keras.layers.Conv2D(64, (3, 3), padding=\"same\"),\n",
        "            keras.layers.Activation(\"relu\"),\n",
        "            keras.layers.BatchNormalization(axis=chanDim, fused=False),\n",
        "            keras.layers.Conv2D(64, (3, 3), padding=\"same\"),\n",
        "            keras.layers.Activation(\"relu\"),\n",
        "            keras.layers.BatchNormalization(axis=chanDim, fused=False),\n",
        "            keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "            keras.layers.Dropout(0.25),\n",
        "            keras.layers.Flatten(),\n",
        "            keras.layers.Dense(512),\n",
        "            keras.layers.Activation(\"relu\"),\n",
        "            keras.layers.BatchNormalization(fused=False),\n",
        "            keras.layers.Dropout(0.5),\n",
        "            keras.layers.Dense(classes),\n",
        "            keras.layers.Activation(\"softmax\")\n",
        "    ])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vk9oFSZXLl-",
        "colab_type": "text"
      },
      "source": [
        "## Train model and save it's checkpoints\n",
        "\n",
        "- Use new Session and Graph to ensure that we can use absolutory same name of variables for train and eval phase.\n",
        "- call `tf.contrib.quantize.create_training_graph` after building model since we want to do Quantization Aware Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "f3202f38-104d-4fbb-a329-e58faca5d2ee",
        "id": "x6XlYG__fBVk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        }
      },
      "source": [
        "# train\n",
        "train_graph = tf.Graph()\n",
        "train_sess = tf.Session(graph=train_graph)\n",
        "\n",
        "\n",
        "keras.backend.set_session(train_sess)\n",
        "with train_graph.as_default():\n",
        "    train_model = build_keras_model()\n",
        "\n",
        "    tf.contrib.quantize.create_training_graph(input_graph=train_graph, quant_delay=100)\n",
        "    train_sess.run(tf.global_variables_initializer())    \n",
        "    \n",
        "    #opt = SGD(lr=INIT_LR, momentum=0.9, decay=INIT_LR / NUM_EPOCHS)\n",
        "    train_model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
        "\t   metrics=[\"accuracy\"])\n",
        "\n",
        "    train_model.fit(trainX, trainY,\n",
        "      validation_data=(testX, testY),\n",
        "      batch_size=BS, epochs=NUM_EPOCHS)    \n",
        "    # save graph and checkpoints\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(train_sess, 'checkpoints')\n",
        "    predictions = train_model.predict(testX)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0903 05:18:06.280748 139652712466304 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0903 05:18:09.226524 139652712466304 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/12\n",
            "60000/60000 [==============================] - 33s 552us/sample - loss: 0.4973 - acc: 0.8308 - val_loss: 2.4466 - val_acc: 0.2204\n",
            "Epoch 2/12\n",
            "60000/60000 [==============================] - 28s 463us/sample - loss: 0.3072 - acc: 0.8883 - val_loss: 1.2981 - val_acc: 0.5891\n",
            "Epoch 3/12\n",
            "60000/60000 [==============================] - 28s 462us/sample - loss: 0.2579 - acc: 0.9066 - val_loss: 0.2733 - val_acc: 0.8977\n",
            "Epoch 4/12\n",
            "60000/60000 [==============================] - 28s 467us/sample - loss: 0.2338 - acc: 0.9140 - val_loss: 0.2243 - val_acc: 0.9206\n",
            "Epoch 5/12\n",
            "60000/60000 [==============================] - 28s 462us/sample - loss: 0.2144 - acc: 0.9208 - val_loss: 0.2215 - val_acc: 0.9210\n",
            "Epoch 6/12\n",
            "60000/60000 [==============================] - 28s 461us/sample - loss: 0.2017 - acc: 0.9260 - val_loss: 0.2102 - val_acc: 0.9240\n",
            "Epoch 7/12\n",
            "60000/60000 [==============================] - 28s 462us/sample - loss: 0.1874 - acc: 0.9320 - val_loss: 0.2392 - val_acc: 0.9155\n",
            "Epoch 8/12\n",
            "60000/60000 [==============================] - 28s 463us/sample - loss: 0.1761 - acc: 0.9355 - val_loss: 0.2043 - val_acc: 0.9281\n",
            "Epoch 9/12\n",
            "60000/60000 [==============================] - 28s 466us/sample - loss: 0.1683 - acc: 0.9375 - val_loss: 0.2104 - val_acc: 0.9292\n",
            "Epoch 10/12\n",
            "60000/60000 [==============================] - 28s 464us/sample - loss: 0.1619 - acc: 0.9397 - val_loss: 0.1908 - val_acc: 0.9333\n",
            "Epoch 11/12\n",
            "60000/60000 [==============================] - 28s 463us/sample - loss: 0.1542 - acc: 0.9431 - val_loss: 0.2041 - val_acc: 0.9296\n",
            "Epoch 12/12\n",
            "60000/60000 [==============================] - 28s 464us/sample - loss: 0.1428 - acc: 0.9470 - val_loss: 0.2109 - val_acc: 0.9269\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7AT9qAxelzx",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWp9_I06ZjDo",
        "colab_type": "code",
        "outputId": "2b678e9e-e38d-462b-df68-0b9f00ae4efa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "print(predictions[imgindex]*100)\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  predictions_array, true_label, img = predictions_array[i], true_label[i], img[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  \n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "  \n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "  \n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(labelNames[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                labelNames[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  predictions_array, true_label = predictions_array[i], true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  \n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')\n",
        "  \n",
        "\n",
        "tr = np.squeeze(testX)\n",
        "print(np.shape(tr))\n",
        "#ts = np.squeeze(test_images)\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(imgindex, predictions, testLabels, tr)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(imgindex, predictions,  testLabels)\n",
        "plt.show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.4616204e-05 2.1930659e-04 9.4616204e-05 1.2521635e-04 4.4186553e-04\n",
            " 9.9974403e+01 8.9028460e-04 1.9416835e-02 3.1416544e-03 1.1782135e-03]\n",
            "(10000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-e6fdd1be42f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mtr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYOBvXw6X03l",
        "colab_type": "text"
      },
      "source": [
        "### Freeze model and save it\n",
        "\n",
        "- Create new Session and Graph\n",
        "- Call `tf.contrib.quantize.create_eval_graph` and get graph_def after building model before saver.restore\n",
        "- Call `saver.restore` to load the trained weights.\n",
        "   - saver.restore may add unneeded variables to the graph. So we have to get the graph_def before save.restore is called.\n",
        "- We can use `tf.graph_util.convert_variables_to_constants` to freeze the graph_def"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tI1s0JngKN0",
        "colab_type": "code",
        "outputId": "a7d02a31-7e42-4c42-e991-8e962803df61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "source": [
        "# eval\n",
        "eval_graph = tf.Graph()\n",
        "eval_sess = tf.Session(graph=eval_graph)\n",
        "\n",
        "keras.backend.set_session(eval_sess)\n",
        "\n",
        "with eval_graph.as_default():\n",
        "    keras.backend.set_learning_phase(0)\n",
        "    eval_model = build_keras_model()\n",
        "    tf.contrib.quantize.create_eval_graph(input_graph=eval_graph)\n",
        "    eval_graph_def = eval_graph.as_graph_def()\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(eval_sess, 'checkpoints')\n",
        "\n",
        "    frozen_graph_def = tf.graph_util.convert_variables_to_constants(\n",
        "        eval_sess,\n",
        "        eval_graph_def,\n",
        "        [eval_model.output.op.name]\n",
        "    )\n",
        "\n",
        "    with open('frozen_model.pb', 'wb') as f:\n",
        "        f.write(frozen_graph_def.SerializeToString())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0903 05:25:05.761829 139652712466304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "W0903 05:25:05.875834 139652712466304 deprecation.py:323] From <ipython-input-8-995fccbe9e12>:17: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
            "W0903 05:25:05.877070 139652712466304 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.compat.v1.graph_util.extract_sub_graph`\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tETygOV_Y_cX",
        "colab_type": "text"
      },
      "source": [
        "### Generate tflite file\n",
        "\n",
        "- use QUANTIZED_UINT8 option\n",
        "- Quantization Aware training adds min/max information. So we don't need  default_ranges_min default_ranges_max \n",
        "- We don't need call freeze_graph.py since the graph is already freezed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSfaOfSTnXUR",
        "colab_type": "code",
        "outputId": "7bbfc332-494a-49a2-8b03-9f24df071656",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def load_graph(frozen_graph_filename):\n",
        "    # We load the protobuf file from the disk and parse it to retrieve the \n",
        "    # unserialized graph_def\n",
        "    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
        "        graph_def = tf.GraphDef()\n",
        "        graph_def.ParseFromString(f.read())\n",
        "\n",
        "    # Then, we import the graph_def into a new Graph and returns it \n",
        "    with tf.Graph().as_default() as graph:\n",
        "        # The name var will prefix every op/nodes in your graph\n",
        "        # Since we load everything in a new graph, this is not needed\n",
        "        tf.import_graph_def(graph_def, name=\"prefix\")\n",
        "    return graph\n",
        "\n",
        "g = load_graph(\"frozen_model.pb\")\n",
        "\n",
        "for op in g .get_operations(): \n",
        "    print(op.name)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prefix/conv2d_input\n",
            "prefix/conv2d/kernel\n",
            "prefix/conv2d/bias\n",
            "prefix/conv2d/Conv2D/ReadVariableOp\n",
            "prefix/conv2d/BiasAdd/ReadVariableOp\n",
            "prefix/batch_normalization/gamma\n",
            "prefix/batch_normalization/beta\n",
            "prefix/batch_normalization/moving_mean\n",
            "prefix/batch_normalization/moving_variance\n",
            "prefix/batch_normalization/batchnorm/ReadVariableOp\n",
            "prefix/batch_normalization/batchnorm/add/y\n",
            "prefix/batch_normalization/batchnorm/add\n",
            "prefix/batch_normalization/batchnorm/Rsqrt\n",
            "prefix/batch_normalization/batchnorm/mul/ReadVariableOp\n",
            "prefix/batch_normalization/batchnorm/mul\n",
            "prefix/batch_normalization/batchnorm/ReadVariableOp_1\n",
            "prefix/batch_normalization/batchnorm/mul_2\n",
            "prefix/batch_normalization/batchnorm/ReadVariableOp_2\n",
            "prefix/batch_normalization/batchnorm/sub\n",
            "prefix/conv2d_1/kernel\n",
            "prefix/conv2d_1/bias\n",
            "prefix/conv2d_1/Conv2D/ReadVariableOp\n",
            "prefix/conv2d_1/BiasAdd/ReadVariableOp\n",
            "prefix/batch_normalization_1/gamma\n",
            "prefix/batch_normalization_1/beta\n",
            "prefix/batch_normalization_1/moving_mean\n",
            "prefix/batch_normalization_1/moving_variance\n",
            "prefix/batch_normalization_1/batchnorm/ReadVariableOp\n",
            "prefix/batch_normalization_1/batchnorm/add/y\n",
            "prefix/batch_normalization_1/batchnorm/add\n",
            "prefix/batch_normalization_1/batchnorm/Rsqrt\n",
            "prefix/batch_normalization_1/batchnorm/mul/ReadVariableOp\n",
            "prefix/batch_normalization_1/batchnorm/mul\n",
            "prefix/batch_normalization_1/batchnorm/ReadVariableOp_1\n",
            "prefix/batch_normalization_1/batchnorm/mul_2\n",
            "prefix/batch_normalization_1/batchnorm/ReadVariableOp_2\n",
            "prefix/batch_normalization_1/batchnorm/sub\n",
            "prefix/conv2d_2/kernel\n",
            "prefix/conv2d_2/bias\n",
            "prefix/conv2d_2/Conv2D/ReadVariableOp\n",
            "prefix/conv2d_2/BiasAdd/ReadVariableOp\n",
            "prefix/batch_normalization_2/gamma\n",
            "prefix/batch_normalization_2/beta\n",
            "prefix/batch_normalization_2/moving_mean\n",
            "prefix/batch_normalization_2/moving_variance\n",
            "prefix/batch_normalization_2/batchnorm/ReadVariableOp\n",
            "prefix/batch_normalization_2/batchnorm/add/y\n",
            "prefix/batch_normalization_2/batchnorm/add\n",
            "prefix/batch_normalization_2/batchnorm/Rsqrt\n",
            "prefix/batch_normalization_2/batchnorm/mul/ReadVariableOp\n",
            "prefix/batch_normalization_2/batchnorm/mul\n",
            "prefix/batch_normalization_2/batchnorm/ReadVariableOp_1\n",
            "prefix/batch_normalization_2/batchnorm/mul_2\n",
            "prefix/batch_normalization_2/batchnorm/ReadVariableOp_2\n",
            "prefix/batch_normalization_2/batchnorm/sub\n",
            "prefix/conv2d_3/kernel\n",
            "prefix/conv2d_3/bias\n",
            "prefix/conv2d_3/Conv2D/ReadVariableOp\n",
            "prefix/conv2d_3/BiasAdd/ReadVariableOp\n",
            "prefix/batch_normalization_3/gamma\n",
            "prefix/batch_normalization_3/beta\n",
            "prefix/batch_normalization_3/moving_mean\n",
            "prefix/batch_normalization_3/moving_variance\n",
            "prefix/batch_normalization_3/batchnorm/ReadVariableOp\n",
            "prefix/batch_normalization_3/batchnorm/add/y\n",
            "prefix/batch_normalization_3/batchnorm/add\n",
            "prefix/batch_normalization_3/batchnorm/Rsqrt\n",
            "prefix/batch_normalization_3/batchnorm/mul/ReadVariableOp\n",
            "prefix/batch_normalization_3/batchnorm/mul\n",
            "prefix/batch_normalization_3/batchnorm/ReadVariableOp_1\n",
            "prefix/batch_normalization_3/batchnorm/mul_2\n",
            "prefix/batch_normalization_3/batchnorm/ReadVariableOp_2\n",
            "prefix/batch_normalization_3/batchnorm/sub\n",
            "prefix/flatten/strided_slice/stack\n",
            "prefix/flatten/strided_slice/stack_1\n",
            "prefix/flatten/strided_slice/stack_2\n",
            "prefix/flatten/Reshape/shape/1\n",
            "prefix/dense/kernel\n",
            "prefix/dense/bias\n",
            "prefix/dense/MatMul/ReadVariableOp\n",
            "prefix/dense/BiasAdd/ReadVariableOp\n",
            "prefix/batch_normalization_4/gamma\n",
            "prefix/batch_normalization_4/beta\n",
            "prefix/batch_normalization_4/moving_mean\n",
            "prefix/batch_normalization_4/moving_variance\n",
            "prefix/batch_normalization_4/batchnorm/ReadVariableOp\n",
            "prefix/batch_normalization_4/batchnorm/add/y\n",
            "prefix/batch_normalization_4/batchnorm/add\n",
            "prefix/batch_normalization_4/batchnorm/Rsqrt\n",
            "prefix/batch_normalization_4/batchnorm/mul/ReadVariableOp\n",
            "prefix/batch_normalization_4/batchnorm/mul\n",
            "prefix/batch_normalization_4/batchnorm/ReadVariableOp_1\n",
            "prefix/batch_normalization_4/batchnorm/mul_2\n",
            "prefix/batch_normalization_4/batchnorm/ReadVariableOp_2\n",
            "prefix/batch_normalization_4/batchnorm/sub\n",
            "prefix/dense_1/kernel\n",
            "prefix/dense_1/bias\n",
            "prefix/dense_1/MatMul/ReadVariableOp\n",
            "prefix/dense_1/BiasAdd/ReadVariableOp\n",
            "prefix/conv2d/weights_quant/min\n",
            "prefix/conv2d/weights_quant/min/read\n",
            "prefix/conv2d/weights_quant/max\n",
            "prefix/conv2d/weights_quant/max/read\n",
            "prefix/conv2d/weights_quant/FakeQuantWithMinMaxVars\n",
            "prefix/conv2d/Conv2D\n",
            "prefix/conv2d/BiasAdd\n",
            "prefix/activation/Relu\n",
            "prefix/conv2d/act_quant/min\n",
            "prefix/conv2d/act_quant/min/read\n",
            "prefix/conv2d/act_quant/max\n",
            "prefix/conv2d/act_quant/max/read\n",
            "prefix/conv2d/act_quant/FakeQuantWithMinMaxVars\n",
            "prefix/batch_normalization/batchnorm/mul_1\n",
            "prefix/conv2d_1/weights_quant/min\n",
            "prefix/conv2d_1/weights_quant/min/read\n",
            "prefix/conv2d_1/weights_quant/max\n",
            "prefix/conv2d_1/weights_quant/max/read\n",
            "prefix/conv2d_1/weights_quant/FakeQuantWithMinMaxVars\n",
            "prefix/conv2d_1/act_quant/min\n",
            "prefix/conv2d_1/act_quant/min/read\n",
            "prefix/conv2d_1/act_quant/max\n",
            "prefix/conv2d_1/act_quant/max/read\n",
            "prefix/conv2d_2/weights_quant/min\n",
            "prefix/conv2d_2/weights_quant/min/read\n",
            "prefix/conv2d_2/weights_quant/max\n",
            "prefix/conv2d_2/weights_quant/max/read\n",
            "prefix/conv2d_2/weights_quant/FakeQuantWithMinMaxVars\n",
            "prefix/conv2d_2/act_quant/min\n",
            "prefix/conv2d_2/act_quant/min/read\n",
            "prefix/conv2d_2/act_quant/max\n",
            "prefix/conv2d_2/act_quant/max/read\n",
            "prefix/conv2d_3/weights_quant/min\n",
            "prefix/conv2d_3/weights_quant/min/read\n",
            "prefix/conv2d_3/weights_quant/max\n",
            "prefix/conv2d_3/weights_quant/max/read\n",
            "prefix/conv2d_3/weights_quant/FakeQuantWithMinMaxVars\n",
            "prefix/conv2d_3/act_quant/min\n",
            "prefix/conv2d_3/act_quant/min/read\n",
            "prefix/conv2d_3/act_quant/max\n",
            "prefix/conv2d_3/act_quant/max/read\n",
            "prefix/dense/weights_quant/min\n",
            "prefix/dense/weights_quant/min/read\n",
            "prefix/dense/weights_quant/max\n",
            "prefix/dense/weights_quant/max/read\n",
            "prefix/dense/weights_quant/FakeQuantWithMinMaxVars\n",
            "prefix/dense/act_quant/min\n",
            "prefix/dense/act_quant/min/read\n",
            "prefix/dense/act_quant/max\n",
            "prefix/dense/act_quant/max/read\n",
            "prefix/dense_1/weights_quant/min\n",
            "prefix/dense_1/weights_quant/min/read\n",
            "prefix/dense_1/weights_quant/max\n",
            "prefix/dense_1/weights_quant/max/read\n",
            "prefix/dense_1/weights_quant/FakeQuantWithMinMaxVars\n",
            "prefix/dense_1/act_quant/min\n",
            "prefix/dense_1/act_quant/min/read\n",
            "prefix/dense_1/act_quant/max\n",
            "prefix/dense_1/act_quant/max/read\n",
            "prefix/batch_normalization/batchnorm/mul_1/activation_Mul_quant/min\n",
            "prefix/batch_normalization/batchnorm/mul_1/activation_Mul_quant/min/read\n",
            "prefix/batch_normalization/batchnorm/mul_1/activation_Mul_quant/max\n",
            "prefix/batch_normalization/batchnorm/mul_1/activation_Mul_quant/max/read\n",
            "prefix/batch_normalization/batchnorm/mul_1/activation_Mul_quant/FakeQuantWithMinMaxVars\n",
            "prefix/batch_normalization/batchnorm/add_1\n",
            "prefix/batch_normalization/batchnorm/add_1/activation_Add_quant/min\n",
            "prefix/batch_normalization/batchnorm/add_1/activation_Add_quant/min/read\n",
            "prefix/batch_normalization/batchnorm/add_1/activation_Add_quant/max\n",
            "prefix/batch_normalization/batchnorm/add_1/activation_Add_quant/max/read\n",
            "prefix/batch_normalization/batchnorm/add_1/activation_Add_quant/FakeQuantWithMinMaxVars\n",
            "prefix/conv2d_1/Conv2D\n",
            "prefix/conv2d_1/BiasAdd\n",
            "prefix/activation_1/Relu\n",
            "prefix/conv2d_1/act_quant/FakeQuantWithMinMaxVars\n",
            "prefix/batch_normalization_1/batchnorm/mul_1\n",
            "prefix/batch_normalization_1/batchnorm/mul_1/activation_Mul_quant/min\n",
            "prefix/batch_normalization_1/batchnorm/mul_1/activation_Mul_quant/min/read\n",
            "prefix/batch_normalization_1/batchnorm/mul_1/activation_Mul_quant/max\n",
            "prefix/batch_normalization_1/batchnorm/mul_1/activation_Mul_quant/max/read\n",
            "prefix/batch_normalization_1/batchnorm/mul_1/activation_Mul_quant/FakeQuantWithMinMaxVars\n",
            "prefix/batch_normalization_1/batchnorm/add_1\n",
            "prefix/batch_normalization_1/batchnorm/add_1/activation_Add_quant/min\n",
            "prefix/batch_normalization_1/batchnorm/add_1/activation_Add_quant/min/read\n",
            "prefix/batch_normalization_1/batchnorm/add_1/activation_Add_quant/max\n",
            "prefix/batch_normalization_1/batchnorm/add_1/activation_Add_quant/max/read\n",
            "prefix/batch_normalization_1/batchnorm/add_1/activation_Add_quant/FakeQuantWithMinMaxVars\n",
            "prefix/max_pooling2d/MaxPool\n",
            "prefix/dropout/Identity\n",
            "prefix/conv2d_2/Conv2D\n",
            "prefix/conv2d_2/BiasAdd\n",
            "prefix/activation_2/Relu\n",
            "prefix/conv2d_2/act_quant/FakeQuantWithMinMaxVars\n",
            "prefix/batch_normalization_2/batchnorm/mul_1\n",
            "prefix/batch_normalization_2/batchnorm/mul_1/activation_Mul_quant/min\n",
            "prefix/batch_normalization_2/batchnorm/mul_1/activation_Mul_quant/min/read\n",
            "prefix/batch_normalization_2/batchnorm/mul_1/activation_Mul_quant/max\n",
            "prefix/batch_normalization_2/batchnorm/mul_1/activation_Mul_quant/max/read\n",
            "prefix/batch_normalization_2/batchnorm/mul_1/activation_Mul_quant/FakeQuantWithMinMaxVars\n",
            "prefix/batch_normalization_2/batchnorm/add_1\n",
            "prefix/batch_normalization_2/batchnorm/add_1/activation_Add_quant/min\n",
            "prefix/batch_normalization_2/batchnorm/add_1/activation_Add_quant/min/read\n",
            "prefix/batch_normalization_2/batchnorm/add_1/activation_Add_quant/max\n",
            "prefix/batch_normalization_2/batchnorm/add_1/activation_Add_quant/max/read\n",
            "prefix/batch_normalization_2/batchnorm/add_1/activation_Add_quant/FakeQuantWithMinMaxVars\n",
            "prefix/conv2d_3/Conv2D\n",
            "prefix/conv2d_3/BiasAdd\n",
            "prefix/activation_3/Relu\n",
            "prefix/conv2d_3/act_quant/FakeQuantWithMinMaxVars\n",
            "prefix/batch_normalization_3/batchnorm/mul_1\n",
            "prefix/batch_normalization_3/batchnorm/mul_1/activation_Mul_quant/min\n",
            "prefix/batch_normalization_3/batchnorm/mul_1/activation_Mul_quant/min/read\n",
            "prefix/batch_normalization_3/batchnorm/mul_1/activation_Mul_quant/max\n",
            "prefix/batch_normalization_3/batchnorm/mul_1/activation_Mul_quant/max/read\n",
            "prefix/batch_normalization_3/batchnorm/mul_1/activation_Mul_quant/FakeQuantWithMinMaxVars\n",
            "prefix/batch_normalization_3/batchnorm/add_1\n",
            "prefix/batch_normalization_3/batchnorm/add_1/activation_Add_quant/min\n",
            "prefix/batch_normalization_3/batchnorm/add_1/activation_Add_quant/min/read\n",
            "prefix/batch_normalization_3/batchnorm/add_1/activation_Add_quant/max\n",
            "prefix/batch_normalization_3/batchnorm/add_1/activation_Add_quant/max/read\n",
            "prefix/batch_normalization_3/batchnorm/add_1/activation_Add_quant/FakeQuantWithMinMaxVars\n",
            "prefix/max_pooling2d_1/MaxPool\n",
            "prefix/dropout_1/Identity\n",
            "prefix/flatten/Shape\n",
            "prefix/flatten/strided_slice\n",
            "prefix/flatten/Reshape/shape\n",
            "prefix/flatten/Reshape\n",
            "prefix/dense/MatMul\n",
            "prefix/dense/BiasAdd\n",
            "prefix/activation_4/Relu\n",
            "prefix/dense/act_quant/FakeQuantWithMinMaxVars\n",
            "prefix/batch_normalization_4/batchnorm/mul_1\n",
            "prefix/batch_normalization_4/batchnorm/mul_1/activation_Mul_quant/min\n",
            "prefix/batch_normalization_4/batchnorm/mul_1/activation_Mul_quant/min/read\n",
            "prefix/batch_normalization_4/batchnorm/mul_1/activation_Mul_quant/max\n",
            "prefix/batch_normalization_4/batchnorm/mul_1/activation_Mul_quant/max/read\n",
            "prefix/batch_normalization_4/batchnorm/mul_1/activation_Mul_quant/FakeQuantWithMinMaxVars\n",
            "prefix/batch_normalization_4/batchnorm/add_1\n",
            "prefix/batch_normalization_4/batchnorm/add_1/activation_Add_quant/min\n",
            "prefix/batch_normalization_4/batchnorm/add_1/activation_Add_quant/min/read\n",
            "prefix/batch_normalization_4/batchnorm/add_1/activation_Add_quant/max\n",
            "prefix/batch_normalization_4/batchnorm/add_1/activation_Add_quant/max/read\n",
            "prefix/batch_normalization_4/batchnorm/add_1/activation_Add_quant/FakeQuantWithMinMaxVars\n",
            "prefix/dropout_2/Identity\n",
            "prefix/dense_1/MatMul\n",
            "prefix/dense_1/BiasAdd\n",
            "prefix/dense_1/act_quant/FakeQuantWithMinMaxVars\n",
            "prefix/activation_5/Softmax\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APsbHmt7izT8",
        "colab_type": "code",
        "outputId": "51218f13-e09d-4080-9113-6ab2954d4747",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        }
      },
      "source": [
        "%%bash\n",
        "\n",
        "tflite_convert \\\n",
        "    --output_file=model.tflite \\\n",
        "    --graph_def_file=frozen_model.pb \\\n",
        "    --inference_type=QUANTIZED_UINT8 \\\n",
        "    --input_arrays=conv2d_input \\\n",
        "    --output_arrays=activation_5/Softmax \\\n",
        "    --mean_values=0 \\\n",
        "    --std_dev_values=255"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-09-03 05:25:25.489733: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-09-03 05:25:25.513716: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-03 05:25:25.514451: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-03 05:25:25.514754: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-03 05:25:25.516048: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-03 05:25:25.517315: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-03 05:25:25.517691: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-03 05:25:25.524248: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-03 05:25:25.525464: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-03 05:25:25.533167: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-03 05:25:25.533318: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-03 05:25:25.534069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-03 05:25:25.534678: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-03 05:25:25.566240: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-09-03 05:25:25.566738: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b1ba34fb80 executing computations on platform Host. Devices:\n",
            "2019-09-03 05:25:25.566767: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-09-03 05:25:25.624491: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-03 05:25:25.625586: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55b1bec44380 executing computations on platform CUDA. Devices:\n",
            "2019-09-03 05:25:25.625706: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-09-03 05:25:25.625976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-03 05:25:25.626656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-03 05:25:25.626763: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-03 05:25:25.626783: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-03 05:25:25.626796: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-03 05:25:25.626809: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-03 05:25:25.626824: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-03 05:25:25.626836: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-03 05:25:25.626850: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-03 05:25:25.626916: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-03 05:25:25.627593: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-03 05:25:25.628247: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-03 05:25:25.634424: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-03 05:25:25.635788: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-03 05:25:25.635847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-03 05:25:25.635879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-03 05:25:25.642949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-03 05:25:25.643708: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-03 05:25:25.644359: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:40] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2019-09-03 05:25:25.644431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9626 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIKWO5MuZk8f",
        "colab_type": "text"
      },
      "source": [
        "### Check generated tflite file.\n",
        ".\n",
        "- Use TFLiteInterpreter to check the generated file is valid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI0zfQTL-p5U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cd8c6158-259e-4b55-9087-0890a8100555"
      },
      "source": [
        "# load TFLite file\n",
        "interpreter = tf.lite.Interpreter(model_path=f'model.tflite')\n",
        "# Allocate memory. \n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# get some informations .\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "print(input_details)\n",
        "print(output_details)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'name': 'conv2d_input', 'index': 34, 'shape': array([ 1, 28, 28,  1], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.003921568859368563, 0)}]\n",
            "[{'name': 'activation_5/Softmax', 'index': 5, 'shape': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0)}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxNqC2aXliMk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "f8f68859-bd2f-4be1-cf87-7edbe1ff16c8"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\t\t\t checkpoints.meta  sample_data\n",
            "checkpoints.data-00000-of-00001  frozen_model.pb\n",
            "checkpoints.index\t\t model.tflite\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16MsnRzrZ0Yk",
        "colab_type": "text"
      },
      "source": [
        "- I'm not sure how to use quantization attribute in input/output_details. But maybe\n",
        "  - If quantization attribute is (a, b), then the input data f should be transform to (f/a + b) and casted to uint8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_O6nfR4XCmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def quantize(detail, data):\n",
        "    shape = detail['shape']\n",
        "    dtype = detail['dtype']\n",
        "    a, b = detail['quantization']\n",
        "    \n",
        "    return (data/a + b).astype(dtype).reshape(shape)\n",
        "\n",
        "\n",
        "def dequantize(detail, data):\n",
        "    a, b = detail['quantization']\n",
        "    \n",
        "    return (data - b)*a"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJuqJna_vgna",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "c6df9024-30df-452c-c4f8-803d23c0a03f"
      },
      "source": [
        "quantized_input = quantize(input_details[0], test_images[:1])\n",
        "interpreter.set_tensor(input_details[0]['index'], quantized_input)\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# The results are stored on 'index' of output_details\n",
        "quantized_output = interpreter.get_tensor(output_details[0]['index'])\n",
        "\n",
        "print('sample result of quantized model')\n",
        "print(dequantize(output_details[0], quantized_output))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-fd61329af939>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquantized_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_details\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantized_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_images' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rFwYYH5aZjy",
        "colab_type": "text"
      },
      "source": [
        "### Compile the tflite file using EdgeTPU Compiler "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmSX6C6RxAZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "26a587b8-2681-451e-a99c-cbc46e3297ec"
      },
      "source": [
        "%%bash\n",
        "\n",
        "edgetpu_compiler 'model.tflite'"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Edge TPU Compiler version 2.0.258810407\n",
            "\n",
            "Model compiled successfully in 90 ms.\n",
            "\n",
            "Input model: model.tflite\n",
            "Input size: 1.61MiB\n",
            "Output model: model_edgetpu.tflite\n",
            "Output size: 1.72MiB\n",
            "On-chip memory available for caching model parameters: 7.82MiB\n",
            "On-chip memory used for caching model parameters: 1.70MiB\n",
            "Off-chip memory used for streaming uncached model parameters: 6.00KiB\n",
            "Number of Edge TPU subgraphs: 1\n",
            "Total number of operations: 19\n",
            "Operation log: model_edgetpu.log\n",
            "See the operation log file for individual operation details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO: Initialized TensorFlow Lite runtime.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ9qGJwZf_AV",
        "colab_type": "text"
      },
      "source": [
        "We can download the generated file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygUHjCLOz41Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('model_edgetpu.tflite')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMFiIwlk3r40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}